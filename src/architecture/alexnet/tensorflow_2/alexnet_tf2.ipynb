{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**: We will use the weights/biases of a pretained caffe model from this [website](http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet architecture:\n",
    "\n",
    "- **5 Convolutional layers**.\n",
    "- **3 Fully connected layers**.\n",
    "- **3 Overlapping Max pooling layers**.\n",
    "- **ReLU** as activation function for hidden layer.\n",
    "\n",
    "    - Avoid vanishing gradients for positive values.\n",
    "    - More computationally efficient to compute than sigmoid and tanh.\n",
    "    - Better convergence performance than sigmoid and tanh.\n",
    "\n",
    "- **Softmax** as activation function for output layer.\n",
    "- **60,000,000 trainable parameters**.\n",
    "- **Cross-entropy** as cost function\n",
    "- **Mini-batch gradient descent with Momentum optimizer**.\n",
    "    - Batch size : 128.\n",
    "    - Momentum = 0.9.\n",
    "    - Weight decay = 0.0005.\n",
    "    - Learning rate: 0.01. Equal learning rate for all layers and diving by 10 when validation error stopped  improving.\n",
    "- **Local Response Normalization** \n",
    "    - it helps with generalization.\n",
    "\n",
    "--- \n",
    "\n",
    "AlexNet details:\n",
    "\n",
    "- Trained with **ILSVRC-2012** dataset (1.2 million training images, 50,000 validation images, and 150,000 testing images.).\n",
    "- Trained on **90 epochs**.\n",
    "- **Weight initialization**: zero-mean Gaussian distribution and a standard deviation of 0.01.\n",
    "- **Bias initialization**: 1 for 2nd/4th/5th conv layers and all fully-connected layers and 0 for remaining layers.\n",
    "\n",
    "--- \n",
    "\n",
    "AlexNet inputs:\n",
    "\n",
    "- **RGB image of size 256 x 256**. If not, training/test set images need to be resized.\n",
    "   - Example: image_size = 1024 x 500 => Smaller dimension is resized to 256 and resulting image is cropped  to obtain a 256 x 256 image.\n",
    " \n",
    "- the RGB image of size 256 x 256 will then be **cropped into 227 x 227** (cf Data Augmentation part). The paper mistakenly says 224 x 224.\n",
    "\n",
    "--- \n",
    "\n",
    "AlexNet is proned to overfit, thus to prevent that:\n",
    "\n",
    "- **Dropout**.\n",
    "    - 50% dropout rate.\n",
    "- **Data Augmentation**.\n",
    "    - **Translations and horizontal reflections (mirroring)**: Extract random 227 x 227 crops from 256 x 256 images.\n",
    "        - Translation on 1 image: (256−227)∗(256−227) = 841 possible images.\n",
    "        - Mirroring : x2 the training set size.\n",
    "        - New training set size = 1.2 millions * 2 * 841 = 1.2 millions * 1682 images.\n",
    "    - **Altering the intensities of RGB channels**: performing PCA on the set of RGB pixel values throughout the ImageNet training set. Doing this approximately captures an important property of natural images: object identity is invariant to changes in the intensity and color of the illumination.\n",
    "\n",
    "**Remark**:\n",
    "\n",
    "- According to the paper, they only trained on 1.2 millions training data without using data augmentation. The reason is the following:\n",
    "    - Suppose they could get 0.001s per forward/backward pass. It will take (0.001 * 1,200,000 * 1682 * 90) / (60 * 60 * 24 * 365) ~= **5.7 years** to train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![legend](../../img/legend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alexnet-model](../../img/alexnet-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noteworthy are the splitting of some of the convolutional layer (layer two, four and five). It has been used to split up the computation between two GPUs (I guess because GPUs weren’t so strong at that time). Even if that might not be necessary today, we have to define the same splitting to reproduce AlexNet results, although we only use one GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (utils.py, line 17)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/sphird/Environements/deep-learning-tf2/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3319\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-3ff2ac98f005>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from utils import *\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/sphird/Documents/Research-Paper-Summary/src/architecture/alexnet/tensorflow2/utils.py\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    if key in ['conv2', 'conv4', 'conv5']:\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Architecture build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Concatenate, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_conv(input_val, name, half, filters, kernel_size, strides=1, padding='valid'):\n",
    "    \"\"\"\n",
    "        Performs a grouped convolution.\n",
    "        \n",
    "        Parameters:\n",
    "        -input_val: previous layer.\n",
    "        -name: name of the convolution.\n",
    "        -half: Number of channels for each convolution.\n",
    "        -filters: Number of filters for each convolution.\n",
    "        -kernel_size: Kernel size used for each convolution.\n",
    "        -strides: stride. Default value is 1.\n",
    "        -padding: 'valid'(default) or 'same'.\n",
    "        \n",
    "        Returns:\n",
    "        -conv: concatenation of the 2 previous convolution layer.\n",
    "        \n",
    "    \"\"\"\n",
    "    input_val_1 = Lambda(lambda x: x[:, :, :, :half])(input_val)\n",
    "    input_val_2 = Lambda(lambda x: x[:, :, :, half:])(input_val)\n",
    "    \n",
    "    conv_1 = Conv2D(filters=filters, \n",
    "                   kernel_size=kernel_size,\n",
    "                   padding=padding,\n",
    "                   activation='relu',\n",
    "                   name=name + '_1')(input_val_1)\n",
    "    \n",
    "    conv_2 = Conv2D(filters=filters, \n",
    "                   kernel_size=kernel_size,\n",
    "                   padding=padding,\n",
    "                   activation='relu',\n",
    "                   name=name + '_2')(input_val_2)\n",
    "    \n",
    "    conv = Concatenate(name=name)([conv_1, conv_2])\n",
    "    \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlexNet():\n",
    "    x = Input((227, 227, 3))\n",
    "    \n",
    "    conv1 = Conv2D(filters=96, \n",
    "                   kernel_size=(11, 11),\n",
    "                   strides=4,\n",
    "                   activation='relu',\n",
    "                   name='conv1')(x)\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=3,\n",
    "                         strides=2)(conv1)\n",
    "     \n",
    "    conv2 = grouped_conv(input_val=pool1,\n",
    "                         name='conv2', \n",
    "                         half=48,\n",
    "                         filters=128,\n",
    "                         kernel_size=5,\n",
    "                         padding='same')\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=3,\n",
    "                         strides=2)(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=384, \n",
    "                   kernel_size=(3, 3),\n",
    "                   padding='same',\n",
    "                   activation='relu',\n",
    "                   name='conv3')(pool2)\n",
    "    \n",
    "    conv4 = grouped_conv(input_val=conv3,\n",
    "                         name='conv4', \n",
    "                         half=192,\n",
    "                         filters=192,\n",
    "                         kernel_size=3,\n",
    "                         padding='same')\n",
    "    \n",
    "    conv5 = grouped_conv(input_val=conv4,\n",
    "                         name='conv5', \n",
    "                         half=192,\n",
    "                         filters=128,\n",
    "                         kernel_size=3,\n",
    "                         padding='same')\n",
    "    \n",
    "    pool5 = MaxPooling2D(pool_size=3,\n",
    "                         strides=2)(conv5)\n",
    "    \n",
    "    flatten = Flatten()(pool5)\n",
    "    \n",
    "    fc6 = Dense(4096, activation='relu', name='fc6')(flatten)\n",
    "    \n",
    "    fc7 = Dense(4096, activation='relu', name='fc7')(fc6)\n",
    "    \n",
    "    fc8 = Dense(1000, activation='softmax', name='fc8')(fc7)\n",
    "    \n",
    "    model = Model(inputs=x, outputs=fc8)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a7c5e71d4fba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlexNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c89f96ae664a>\u001b[0m in \u001b[0;36mAlexNet\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mAlexNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m227\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m227\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     conv1 = Conv2D(filters=96, \n\u001b[1;32m      5\u001b[0m                    \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "model = AlexNet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Loading pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/bvlc_alexnet.npy', 'bvlc_alexnet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained weights/biases into 'dic' variable.\n",
    "dic = np.load('bvlc_alexnet.npy', encoding = 'bytes', allow_pickle=True).item()\n",
    "# Load weights/biases to model.\n",
    "model = load_weights(model, dic)\n",
    "# Check if properly loaded.\n",
    "check_loaded(model, dic)\n",
    "# Remove 'bvlc_alexnet.npy'.\n",
    "os.remove('bvlc_alexnet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-tf2",
   "language": "python",
   "name": "dl-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
