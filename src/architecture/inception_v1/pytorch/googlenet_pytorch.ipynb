{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The paper [Going Deeper with Convolutions][paper] introduces the first version of Inception model called GoogLeNet.\n",
    "\n",
    "\n",
    "- During ILSVLC-2014, they achieved 1st place at the classification task (top-5 test error = 6.67%)\n",
    "\n",
    "\n",
    "- It has around 6.7977 million parameters which is 9x fewer than AlexNet (ILSVRC-2012 winner) and 20x fewer than its competitor VGG-16.\n",
    "\n",
    "\n",
    "- In most of the standard network architectures, the intuition is not clear why and when to perform the max-pooling operation, when to use the convolutional operation. For example, in AlextNet we have the convolutional operation and max-pooling operation following each other whereas in VGGNet, we have 3 convolutional operations in a row and then 1 max-pooling layer.\n",
    "\n",
    "\n",
    "- Thus, **the idea behind GoogLeNet is to use all the operations at the same time**. It computes multiple kernels of different size over the same input map in parallel, concatenating their results into a single output. This is called an **Inception module**.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/676833120053493770/689776141221101629/unknown.png\"\n",
    "         height=\"100%\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "- Consider the following:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://media.discordapp.net/attachments/676833120053493770/690136206092402715/unknown.png\"\n",
    "         height=\"50%\" width=\"70%\">\n",
    "</div>\n",
    " \n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/676833120053493770/690138280058028146/unknown.png\"\n",
    "         height=\"50%\" width=\"90%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "- The Naive approach is computationally expensive:\n",
    "    - Computation cost = ((28 x 28 x 5 x 5) x 192) x 32 $\\simeq$ **120 Mil**\n",
    "        - We perform (28 x 28 x 5 x 5) operations along 192 channels for each of the 32 filters.\n",
    "\n",
    "\n",
    "- The dimension reduction approach is **less** computationally expensive:\n",
    "    - 1st layer computation cost = ((28 x 28 x 1 x 1) x 192) x 16 $\\simeq$ 2.4 Mil\n",
    "    - 2nd layer computation cost = ((28 x 28 x 5 x 5) x 16) x 32 $\\simeq$ 10 Mil  \n",
    "    - Total computation cost $\\simeq$ **12.4 Mil**\n",
    "\n",
    "---\n",
    "\n",
    "Here its architecture:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/676833120053493770/690150147392667651/unknown.png\"\n",
    "         height=\"100%\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "- There are:\n",
    "    - 9 Inception modules (red box)\n",
    "    - Global Average pooling were used instead of a Fully-connected layer.\n",
    "        - It enables adapting and fine-tuning on the network easily.\n",
    "    - 2 auxilaries softmax layer (green box)\n",
    "        - Their role is to push the network toward its goal and helps to ensure that the intermediate features are good enough for the network to learn.\n",
    "        - It turns out that softmax0 and sofmax1 gives regularization effect.\n",
    "        - They are discarded during inference.\n",
    "        - Structure:\n",
    "            - Average pooling layer with 5Ã—5 filter size and stride 3 resulting in an output size:\n",
    "                - For 1st green box: 4x4x512.\n",
    "                - For 2nd green box: 4x4x528.\n",
    "            - 128 1x1 convolutions + ReLU.\n",
    "            - Fully-connected layer with 1024 units + ReLU.\n",
    "            - Dropout = 70%.\n",
    "            - Linear layer (1000 classes) + Softmax.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/676833120053493770/690147584534511659/unknown.png\"\n",
    "         height=\"100%\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "[paper]: https://arxiv.org/pdf/1409.4842.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Auxilaries softmax will note be implemented since pretrained weights will be loaded.\n",
    "- Local Response Normalization will be replaced by Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from classes import class_names\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import urllib.request\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Architecture build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvB(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ConvB, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2D(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionB(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(InceptionB, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        \n",
    "       \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
