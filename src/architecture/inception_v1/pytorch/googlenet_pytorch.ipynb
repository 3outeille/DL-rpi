{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The paper [Going Deeper with Convolutions][paper] introduces the first version of Inception model called GoogLeNet.\n",
    "\n",
    "\n",
    "- During ILSVLC-2014, they achieved 1st place at the classification task (top-5 test error = 6.67%)\n",
    "\n",
    "\n",
    "- It has around 6.7977 million parameters (without auxilaries layers) which is 9x fewer than AlexNet (ILSVRC-2012 winner) and 20x fewer than its competitor VGG-16.\n",
    "\n",
    "\n",
    "- In most of the standard network architectures, the intuition is not clear why and when to perform the max-pooling operation, when to use the convolutional operation. For example, in AlextNet we have the convolutional operation and max-pooling operation following each other whereas in VGGNet, we have 3 convolutional operations in a row and then 1 max-pooling layer.\n",
    "\n",
    "\n",
    "- Thus, **the idea behind GoogLeNet is to use all the operations at the same time**. It computes multiple kernels of different size over the same input map in parallel, concatenating their results into a single output. This is called an **Inception module**.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/676833120053493770/689776141221101629/unknown.png\"\n",
    "         height=\"100%\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "- Consider the following:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://media.discordapp.net/attachments/676833120053493770/690136206092402715/unknown.png\"\n",
    "         height=\"50%\" width=\"70%\">\n",
    "</div>\n",
    " \n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/676833120053493770/690138280058028146/unknown.png\"\n",
    "         height=\"50%\" width=\"90%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "- The Naive approach is computationally expensive:\n",
    "    - Computation cost = ((28 x 28 x 5 x 5) x 192) x 32 $\\simeq$ **120 Mil**\n",
    "        - We perform (28 x 28 x 5 x 5) operations along 192 channels for each of the 32 filters.\n",
    "\n",
    "\n",
    "- The dimension reduction approach is **less** computationally expensive:\n",
    "    - 1st layer computation cost = ((28 x 28 x 1 x 1) x 192) x 16 $\\simeq$ 2.4 Mil\n",
    "    - 2nd layer computation cost = ((28 x 28 x 5 x 5) x 16) x 32 $\\simeq$ 10 Mil  \n",
    "    - Total computation cost $\\simeq$ **12.4 Mil**\n",
    "\n",
    "---\n",
    "\n",
    "Here its architecture:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/676833120053493770/690150147392667651/unknown.png\"\n",
    "         height=\"100%\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "- There are:\n",
    "    - 9 Inception modules (red box)\n",
    "    - Global Average pooling were used instead of a Fully-connected layer.\n",
    "        - It enables adapting and fine-tuning on the network easily.\n",
    "    - 2 auxilaries softmax layer (green box)\n",
    "        - Their role is to push the network toward its goal and helps to ensure that the intermediate features are good enough for the network to learn.\n",
    "        - It turns out that softmax0 and sofmax1 gives regularization effect.\n",
    "        - During training, their loss gets added to the total loss with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3).\n",
    "        - During inference, they are discarded.\n",
    "        - Structure:\n",
    "            - Average pooling layer with 5Ã—5 filter size and stride 3 resulting in an output size:\n",
    "                - For 1st green box: 4x4x512.\n",
    "                - For 2nd green box: 4x4x528.\n",
    "            - 128 1x1 convolutions + ReLU.\n",
    "            - Fully-connected layer with 1024 units + ReLU.\n",
    "            - Dropout = 70%.\n",
    "            - Linear layer (1000 classes) + Softmax.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/676833120053493770/690147584534511659/unknown.png\"\n",
    "         height=\"100%\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/676833120053493770/690676447336988832/legend.png\"\n",
    "         height=\"100%\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/676833120053493770/691604662268461056/unknown.png\"\n",
    "         height=\"100%\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/676833120053493770/691591126502866944/unknown.png\"\n",
    "         height=\"100%\" width=\"100%\">\n",
    "</div>\n",
    "\n",
    "[paper]: https://arxiv.org/pdf/1409.4842.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from classes import class_names\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import urllib.request\n",
    "import scipy.stats as stats\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Loading dataset / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar():\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "            \n",
    "    train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    #Clear downloading message.\n",
    "    clear_output()\n",
    "    \n",
    "    # Split dataset into training set and validation set.\n",
    "    train_dataset, val_dataset = random_split(train_dataset, (45000, 5000))\n",
    "    \n",
    "    print(\"Image Shape: {}\".format(train_dataset[0][0].numpy().shape), end = '\\n\\n')\n",
    "    print(\"Training Set:   {} samples\".format(len(train_dataset)))\n",
    "    print(\"Validation Set:   {} samples\".format(len(val_dataset)))\n",
    "    print(\"Test Set:       {} samples\".format(len(test_dataset)))\n",
    "    \n",
    "    # Create iterator.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)\n",
    "    \n",
    "    # Delete the data/ folder.\n",
    "    shutil.rmtree('./data')\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: (3, 32, 32)\n",
      "\n",
      "Training Set:   45000 samples\n",
      "Validation Set:   5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = load_cifar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Architecture build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Local Response Normalization will be replaced by Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, f_1x1, f_3x3_r, f_3x3, f_5x5_r, f_5x5, f_pp):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvBlock(in_channels, f_1x1, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_channels, f_3x3_r, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(f_3x3_r, f_3x3, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBlock(in_channels, f_5x5_r, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(f_5x5_r, f_5x5, kernel_size=5, stride=1, padding=2)\n",
    "        )\n",
    "        \n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1, ceil_mode=True),\n",
    "            ConvBlock(in_channels, f_pp, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        \n",
    "        return torch.cat([branch1, branch2, branch3, branch4], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionAux(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((4,4))\n",
    "        self.conv = nn.Conv2d(in_channels, 128, kernel_size=1, stride=1, padding=0)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        x = self.act(x)\n",
    "    \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, isTraining, num_classes = 1000, init_weights=True):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        \n",
    "        self.isTraining = isTraining\n",
    "        \n",
    "        self.conv1 = ConvBlock(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.pool1 = nn.MaxPool2d(3, stride=2, padding=0, ceil_mode=True)\n",
    "        self.conv2 = ConvBlock(64, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv3 = ConvBlock(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(3, stride=2, padding=0, ceil_mode=True)\n",
    "        self.inception3A = InceptionModule(in_channels=192,\n",
    "                                           f_1x1=64,\n",
    "                                           f_3x3_r=96,\n",
    "                                           f_3x3=128,\n",
    "                                           f_5x5_r=16,\n",
    "                                           f_5x5=32,\n",
    "                                           f_pp=32)\n",
    "        self.inception3B = InceptionModule(in_channels=256,\n",
    "                                           f_1x1=128,\n",
    "                                           f_3x3_r=128,\n",
    "                                           f_3x3=192,\n",
    "                                           f_5x5_r=32,\n",
    "                                           f_5x5=96,\n",
    "                                           f_pp=64)\n",
    "        self.pool4 = nn.MaxPool2d(3, stride=2, padding=0, ceil_mode=True)\n",
    "        self.inception4A = InceptionModule(in_channels=480,\n",
    "                                           f_1x1=192,\n",
    "                                           f_3x3_r=96,\n",
    "                                           f_3x3=208,\n",
    "                                           f_5x5_r=16,\n",
    "                                           f_5x5=48,\n",
    "                                           f_pp=64)\n",
    "        self.inception4B = InceptionModule(in_channels=512,\n",
    "                                           f_1x1=160,\n",
    "                                           f_3x3_r=112,\n",
    "                                           f_3x3=224,\n",
    "                                           f_5x5_r=24,\n",
    "                                           f_5x5=64,\n",
    "                                           f_pp=64)\n",
    "        self.inception4C = InceptionModule(in_channels=512,\n",
    "                                           f_1x1=128,\n",
    "                                           f_3x3_r=128,\n",
    "                                           f_3x3=256,\n",
    "                                           f_5x5_r=24,\n",
    "                                           f_5x5=64,\n",
    "                                           f_pp=64)\n",
    "        self.inception4D = InceptionModule(in_channels=512,\n",
    "                                           f_1x1=112,\n",
    "                                           f_3x3_r=144,\n",
    "                                           f_3x3=288,\n",
    "                                           f_5x5_r=32,\n",
    "                                           f_5x5=64,\n",
    "                                           f_pp=64)\n",
    "        self.inception4E = InceptionModule(in_channels=528,\n",
    "                                           f_1x1=256,\n",
    "                                           f_3x3_r=160,\n",
    "                                           f_3x3=320,\n",
    "                                           f_5x5_r=32,\n",
    "                                           f_5x5=128,\n",
    "                                           f_pp=128)\n",
    "        self.pool5 = nn.MaxPool2d(3, stride=2, padding=0, ceil_mode=True)\n",
    "        self.inception5A = InceptionModule(in_channels=832,\n",
    "                                           f_1x1=256,\n",
    "                                           f_3x3_r=160,\n",
    "                                           f_3x3=320,\n",
    "                                           f_5x5_r=32,\n",
    "                                           f_5x5=128,\n",
    "                                           f_pp=128)\n",
    "        self.inception5B = InceptionModule(in_channels=832,\n",
    "                                           f_1x1=384,\n",
    "                                           f_3x3_r=192,\n",
    "                                           f_3x3=384,\n",
    "                                           f_5x5_r=48,\n",
    "                                           f_5x5=128,\n",
    "                                           f_pp=128)\n",
    "        self.pool6 = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        if self.isTraining:   \n",
    "            self.aux4A = InceptionAux(512, num_classes) \n",
    "            self.aux4D = InceptionAux(528, num_classes)\n",
    "        \n",
    "#         if init_weights:\n",
    "#             self._initialize_weights()\n",
    "    \n",
    "#     def _initialize_weights(self):\n",
    "#         for m in self.modules():\n",
    "#             print(m)\n",
    "#             if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "#                 X = stats.truncnorm(-2, 2, scale=0.01)\n",
    "#                 values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "#                 values = values.view(m.weight.size())\n",
    "#                 with torch.no_grad():\n",
    "#                     m.weight.copy_(values)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 nn.init.constant_(m.weight, 1)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        print(x.shape)\n",
    "        x = self.inception3A(x)\n",
    "        print(x.shape)\n",
    "        x = self.inception3B(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.inception4A(x)\n",
    "        \n",
    "        if self.isTraining:\n",
    "            aux1 = self.aux4A(x)\n",
    "        \n",
    "        x = self.inception4B(x)\n",
    "        x = self.inception4C(x)\n",
    "        x = self.inception4D(x)\n",
    "        \n",
    "        if self.isTraining:\n",
    "            aux2 = self.aux4D(x)\n",
    "        \n",
    "        x = self.inception4E(x)\n",
    "        x = self.pool5(x)\n",
    "        x = self.inception5A(x)\n",
    "        x = self.inception5B(x)\n",
    "        x = self.pool6(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if self.isTraining:\n",
    "            return x, aux1, aux2\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GoogLeNet(isTraining=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 192, 28, 28])\n",
      "torch.Size([2, 256, 28, 28])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,472\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         ConvBlock-4         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-5           [-1, 64, 56, 56]               0\n",
      "            Conv2d-6           [-1, 64, 56, 56]           4,160\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "              ReLU-8           [-1, 64, 56, 56]               0\n",
      "         ConvBlock-9           [-1, 64, 56, 56]               0\n",
      "           Conv2d-10          [-1, 192, 56, 56]         110,784\n",
      "      BatchNorm2d-11          [-1, 192, 56, 56]             384\n",
      "             ReLU-12          [-1, 192, 56, 56]               0\n",
      "        ConvBlock-13          [-1, 192, 56, 56]               0\n",
      "        MaxPool2d-14          [-1, 192, 28, 28]               0\n",
      "           Conv2d-15           [-1, 64, 28, 28]          12,352\n",
      "      BatchNorm2d-16           [-1, 64, 28, 28]             128\n",
      "             ReLU-17           [-1, 64, 28, 28]               0\n",
      "        ConvBlock-18           [-1, 64, 28, 28]               0\n",
      "           Conv2d-19           [-1, 96, 28, 28]          18,528\n",
      "      BatchNorm2d-20           [-1, 96, 28, 28]             192\n",
      "             ReLU-21           [-1, 96, 28, 28]               0\n",
      "        ConvBlock-22           [-1, 96, 28, 28]               0\n",
      "           Conv2d-23          [-1, 128, 28, 28]         110,720\n",
      "      BatchNorm2d-24          [-1, 128, 28, 28]             256\n",
      "             ReLU-25          [-1, 128, 28, 28]               0\n",
      "        ConvBlock-26          [-1, 128, 28, 28]               0\n",
      "           Conv2d-27           [-1, 16, 28, 28]           3,088\n",
      "      BatchNorm2d-28           [-1, 16, 28, 28]              32\n",
      "             ReLU-29           [-1, 16, 28, 28]               0\n",
      "        ConvBlock-30           [-1, 16, 28, 28]               0\n",
      "           Conv2d-31           [-1, 32, 28, 28]          12,832\n",
      "      BatchNorm2d-32           [-1, 32, 28, 28]              64\n",
      "             ReLU-33           [-1, 32, 28, 28]               0\n",
      "        ConvBlock-34           [-1, 32, 28, 28]               0\n",
      "        MaxPool2d-35          [-1, 192, 28, 28]               0\n",
      "           Conv2d-36           [-1, 32, 28, 28]           6,176\n",
      "      BatchNorm2d-37           [-1, 32, 28, 28]              64\n",
      "             ReLU-38           [-1, 32, 28, 28]               0\n",
      "        ConvBlock-39           [-1, 32, 28, 28]               0\n",
      "  InceptionModule-40          [-1, 256, 28, 28]               0\n",
      "           Conv2d-41          [-1, 128, 28, 28]          32,896\n",
      "      BatchNorm2d-42          [-1, 128, 28, 28]             256\n",
      "             ReLU-43          [-1, 128, 28, 28]               0\n",
      "        ConvBlock-44          [-1, 128, 28, 28]               0\n",
      "           Conv2d-45          [-1, 128, 28, 28]          32,896\n",
      "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
      "             ReLU-47          [-1, 128, 28, 28]               0\n",
      "        ConvBlock-48          [-1, 128, 28, 28]               0\n",
      "           Conv2d-49          [-1, 192, 28, 28]         221,376\n",
      "      BatchNorm2d-50          [-1, 192, 28, 28]             384\n",
      "             ReLU-51          [-1, 192, 28, 28]               0\n",
      "        ConvBlock-52          [-1, 192, 28, 28]               0\n",
      "           Conv2d-53           [-1, 32, 28, 28]           8,224\n",
      "      BatchNorm2d-54           [-1, 32, 28, 28]              64\n",
      "             ReLU-55           [-1, 32, 28, 28]               0\n",
      "        ConvBlock-56           [-1, 32, 28, 28]               0\n",
      "           Conv2d-57           [-1, 96, 28, 28]          76,896\n",
      "      BatchNorm2d-58           [-1, 96, 28, 28]             192\n",
      "             ReLU-59           [-1, 96, 28, 28]               0\n",
      "        ConvBlock-60           [-1, 96, 28, 28]               0\n",
      "        MaxPool2d-61          [-1, 256, 28, 28]               0\n",
      "           Conv2d-62           [-1, 64, 28, 28]          16,448\n",
      "      BatchNorm2d-63           [-1, 64, 28, 28]             128\n",
      "             ReLU-64           [-1, 64, 28, 28]               0\n",
      "        ConvBlock-65           [-1, 64, 28, 28]               0\n",
      "  InceptionModule-66          [-1, 480, 28, 28]               0\n",
      "        MaxPool2d-67          [-1, 480, 14, 14]               0\n",
      "           Conv2d-68          [-1, 192, 14, 14]          92,352\n",
      "      BatchNorm2d-69          [-1, 192, 14, 14]             384\n",
      "             ReLU-70          [-1, 192, 14, 14]               0\n",
      "        ConvBlock-71          [-1, 192, 14, 14]               0\n",
      "           Conv2d-72           [-1, 96, 14, 14]          46,176\n",
      "      BatchNorm2d-73           [-1, 96, 14, 14]             192\n",
      "             ReLU-74           [-1, 96, 14, 14]               0\n",
      "        ConvBlock-75           [-1, 96, 14, 14]               0\n",
      "           Conv2d-76          [-1, 208, 14, 14]         179,920\n",
      "      BatchNorm2d-77          [-1, 208, 14, 14]             416\n",
      "             ReLU-78          [-1, 208, 14, 14]               0\n",
      "        ConvBlock-79          [-1, 208, 14, 14]               0\n",
      "           Conv2d-80           [-1, 16, 14, 14]           7,696\n",
      "      BatchNorm2d-81           [-1, 16, 14, 14]              32\n",
      "             ReLU-82           [-1, 16, 14, 14]               0\n",
      "        ConvBlock-83           [-1, 16, 14, 14]               0\n",
      "           Conv2d-84           [-1, 48, 14, 14]          19,248\n",
      "      BatchNorm2d-85           [-1, 48, 14, 14]              96\n",
      "             ReLU-86           [-1, 48, 14, 14]               0\n",
      "        ConvBlock-87           [-1, 48, 14, 14]               0\n",
      "        MaxPool2d-88          [-1, 480, 14, 14]               0\n",
      "           Conv2d-89           [-1, 64, 14, 14]          30,784\n",
      "      BatchNorm2d-90           [-1, 64, 14, 14]             128\n",
      "             ReLU-91           [-1, 64, 14, 14]               0\n",
      "        ConvBlock-92           [-1, 64, 14, 14]               0\n",
      "  InceptionModule-93          [-1, 512, 14, 14]               0\n",
      "           Conv2d-94          [-1, 160, 14, 14]          82,080\n",
      "      BatchNorm2d-95          [-1, 160, 14, 14]             320\n",
      "             ReLU-96          [-1, 160, 14, 14]               0\n",
      "        ConvBlock-97          [-1, 160, 14, 14]               0\n",
      "           Conv2d-98          [-1, 112, 14, 14]          57,456\n",
      "      BatchNorm2d-99          [-1, 112, 14, 14]             224\n",
      "            ReLU-100          [-1, 112, 14, 14]               0\n",
      "       ConvBlock-101          [-1, 112, 14, 14]               0\n",
      "          Conv2d-102          [-1, 224, 14, 14]         226,016\n",
      "     BatchNorm2d-103          [-1, 224, 14, 14]             448\n",
      "            ReLU-104          [-1, 224, 14, 14]               0\n",
      "       ConvBlock-105          [-1, 224, 14, 14]               0\n",
      "          Conv2d-106           [-1, 24, 14, 14]          12,312\n",
      "     BatchNorm2d-107           [-1, 24, 14, 14]              48\n",
      "            ReLU-108           [-1, 24, 14, 14]               0\n",
      "       ConvBlock-109           [-1, 24, 14, 14]               0\n",
      "          Conv2d-110           [-1, 64, 14, 14]          38,464\n",
      "     BatchNorm2d-111           [-1, 64, 14, 14]             128\n",
      "            ReLU-112           [-1, 64, 14, 14]               0\n",
      "       ConvBlock-113           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-114          [-1, 512, 14, 14]               0\n",
      "          Conv2d-115           [-1, 64, 14, 14]          32,832\n",
      "     BatchNorm2d-116           [-1, 64, 14, 14]             128\n",
      "            ReLU-117           [-1, 64, 14, 14]               0\n",
      "       ConvBlock-118           [-1, 64, 14, 14]               0\n",
      " InceptionModule-119          [-1, 512, 14, 14]               0\n",
      "          Conv2d-120          [-1, 128, 14, 14]          65,664\n",
      "     BatchNorm2d-121          [-1, 128, 14, 14]             256\n",
      "            ReLU-122          [-1, 128, 14, 14]               0\n",
      "       ConvBlock-123          [-1, 128, 14, 14]               0\n",
      "          Conv2d-124          [-1, 128, 14, 14]          65,664\n",
      "     BatchNorm2d-125          [-1, 128, 14, 14]             256\n",
      "            ReLU-126          [-1, 128, 14, 14]               0\n",
      "       ConvBlock-127          [-1, 128, 14, 14]               0\n",
      "          Conv2d-128          [-1, 256, 14, 14]         295,168\n",
      "     BatchNorm2d-129          [-1, 256, 14, 14]             512\n",
      "            ReLU-130          [-1, 256, 14, 14]               0\n",
      "       ConvBlock-131          [-1, 256, 14, 14]               0\n",
      "          Conv2d-132           [-1, 24, 14, 14]          12,312\n",
      "     BatchNorm2d-133           [-1, 24, 14, 14]              48\n",
      "            ReLU-134           [-1, 24, 14, 14]               0\n",
      "       ConvBlock-135           [-1, 24, 14, 14]               0\n",
      "          Conv2d-136           [-1, 64, 14, 14]          38,464\n",
      "     BatchNorm2d-137           [-1, 64, 14, 14]             128\n",
      "            ReLU-138           [-1, 64, 14, 14]               0\n",
      "       ConvBlock-139           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-140          [-1, 512, 14, 14]               0\n",
      "          Conv2d-141           [-1, 64, 14, 14]          32,832\n",
      "     BatchNorm2d-142           [-1, 64, 14, 14]             128\n",
      "            ReLU-143           [-1, 64, 14, 14]               0\n",
      "       ConvBlock-144           [-1, 64, 14, 14]               0\n",
      " InceptionModule-145          [-1, 512, 14, 14]               0\n",
      "          Conv2d-146          [-1, 112, 14, 14]          57,456\n",
      "     BatchNorm2d-147          [-1, 112, 14, 14]             224\n",
      "            ReLU-148          [-1, 112, 14, 14]               0\n",
      "       ConvBlock-149          [-1, 112, 14, 14]               0\n",
      "          Conv2d-150          [-1, 144, 14, 14]          73,872\n",
      "     BatchNorm2d-151          [-1, 144, 14, 14]             288\n",
      "            ReLU-152          [-1, 144, 14, 14]               0\n",
      "       ConvBlock-153          [-1, 144, 14, 14]               0\n",
      "          Conv2d-154          [-1, 288, 14, 14]         373,536\n",
      "     BatchNorm2d-155          [-1, 288, 14, 14]             576\n",
      "            ReLU-156          [-1, 288, 14, 14]               0\n",
      "       ConvBlock-157          [-1, 288, 14, 14]               0\n",
      "          Conv2d-158           [-1, 32, 14, 14]          16,416\n",
      "     BatchNorm2d-159           [-1, 32, 14, 14]              64\n",
      "            ReLU-160           [-1, 32, 14, 14]               0\n",
      "       ConvBlock-161           [-1, 32, 14, 14]               0\n",
      "          Conv2d-162           [-1, 64, 14, 14]          51,264\n",
      "     BatchNorm2d-163           [-1, 64, 14, 14]             128\n",
      "            ReLU-164           [-1, 64, 14, 14]               0\n",
      "       ConvBlock-165           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-166          [-1, 512, 14, 14]               0\n",
      "          Conv2d-167           [-1, 64, 14, 14]          32,832\n",
      "     BatchNorm2d-168           [-1, 64, 14, 14]             128\n",
      "            ReLU-169           [-1, 64, 14, 14]               0\n",
      "       ConvBlock-170           [-1, 64, 14, 14]               0\n",
      " InceptionModule-171          [-1, 528, 14, 14]               0\n",
      "          Conv2d-172          [-1, 256, 14, 14]         135,424\n",
      "     BatchNorm2d-173          [-1, 256, 14, 14]             512\n",
      "            ReLU-174          [-1, 256, 14, 14]               0\n",
      "       ConvBlock-175          [-1, 256, 14, 14]               0\n",
      "          Conv2d-176          [-1, 160, 14, 14]          84,640\n",
      "     BatchNorm2d-177          [-1, 160, 14, 14]             320\n",
      "            ReLU-178          [-1, 160, 14, 14]               0\n",
      "       ConvBlock-179          [-1, 160, 14, 14]               0\n",
      "          Conv2d-180          [-1, 320, 14, 14]         461,120\n",
      "     BatchNorm2d-181          [-1, 320, 14, 14]             640\n",
      "            ReLU-182          [-1, 320, 14, 14]               0\n",
      "       ConvBlock-183          [-1, 320, 14, 14]               0\n",
      "          Conv2d-184           [-1, 32, 14, 14]          16,928\n",
      "     BatchNorm2d-185           [-1, 32, 14, 14]              64\n",
      "            ReLU-186           [-1, 32, 14, 14]               0\n",
      "       ConvBlock-187           [-1, 32, 14, 14]               0\n",
      "          Conv2d-188          [-1, 128, 14, 14]         102,528\n",
      "     BatchNorm2d-189          [-1, 128, 14, 14]             256\n",
      "            ReLU-190          [-1, 128, 14, 14]               0\n",
      "       ConvBlock-191          [-1, 128, 14, 14]               0\n",
      "       MaxPool2d-192          [-1, 528, 14, 14]               0\n",
      "          Conv2d-193          [-1, 128, 14, 14]          67,712\n",
      "     BatchNorm2d-194          [-1, 128, 14, 14]             256\n",
      "            ReLU-195          [-1, 128, 14, 14]               0\n",
      "       ConvBlock-196          [-1, 128, 14, 14]               0\n",
      " InceptionModule-197          [-1, 832, 14, 14]               0\n",
      "       MaxPool2d-198            [-1, 832, 7, 7]               0\n",
      "          Conv2d-199            [-1, 256, 7, 7]         213,248\n",
      "     BatchNorm2d-200            [-1, 256, 7, 7]             512\n",
      "            ReLU-201            [-1, 256, 7, 7]               0\n",
      "       ConvBlock-202            [-1, 256, 7, 7]               0\n",
      "          Conv2d-203            [-1, 160, 7, 7]         133,280\n",
      "     BatchNorm2d-204            [-1, 160, 7, 7]             320\n",
      "            ReLU-205            [-1, 160, 7, 7]               0\n",
      "       ConvBlock-206            [-1, 160, 7, 7]               0\n",
      "          Conv2d-207            [-1, 320, 7, 7]         461,120\n",
      "     BatchNorm2d-208            [-1, 320, 7, 7]             640\n",
      "            ReLU-209            [-1, 320, 7, 7]               0\n",
      "       ConvBlock-210            [-1, 320, 7, 7]               0\n",
      "          Conv2d-211             [-1, 32, 7, 7]          26,656\n",
      "     BatchNorm2d-212             [-1, 32, 7, 7]              64\n",
      "            ReLU-213             [-1, 32, 7, 7]               0\n",
      "       ConvBlock-214             [-1, 32, 7, 7]               0\n",
      "          Conv2d-215            [-1, 128, 7, 7]         102,528\n",
      "     BatchNorm2d-216            [-1, 128, 7, 7]             256\n",
      "            ReLU-217            [-1, 128, 7, 7]               0\n",
      "       ConvBlock-218            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-219            [-1, 832, 7, 7]               0\n",
      "          Conv2d-220            [-1, 128, 7, 7]         106,624\n",
      "     BatchNorm2d-221            [-1, 128, 7, 7]             256\n",
      "            ReLU-222            [-1, 128, 7, 7]               0\n",
      "       ConvBlock-223            [-1, 128, 7, 7]               0\n",
      " InceptionModule-224            [-1, 832, 7, 7]               0\n",
      "          Conv2d-225            [-1, 384, 7, 7]         319,872\n",
      "     BatchNorm2d-226            [-1, 384, 7, 7]             768\n",
      "            ReLU-227            [-1, 384, 7, 7]               0\n",
      "       ConvBlock-228            [-1, 384, 7, 7]               0\n",
      "          Conv2d-229            [-1, 192, 7, 7]         159,936\n",
      "     BatchNorm2d-230            [-1, 192, 7, 7]             384\n",
      "            ReLU-231            [-1, 192, 7, 7]               0\n",
      "       ConvBlock-232            [-1, 192, 7, 7]               0\n",
      "          Conv2d-233            [-1, 384, 7, 7]         663,936\n",
      "     BatchNorm2d-234            [-1, 384, 7, 7]             768\n",
      "            ReLU-235            [-1, 384, 7, 7]               0\n",
      "       ConvBlock-236            [-1, 384, 7, 7]               0\n",
      "          Conv2d-237             [-1, 48, 7, 7]          39,984\n",
      "     BatchNorm2d-238             [-1, 48, 7, 7]              96\n",
      "            ReLU-239             [-1, 48, 7, 7]               0\n",
      "       ConvBlock-240             [-1, 48, 7, 7]               0\n",
      "          Conv2d-241            [-1, 128, 7, 7]         153,728\n",
      "     BatchNorm2d-242            [-1, 128, 7, 7]             256\n",
      "            ReLU-243            [-1, 128, 7, 7]               0\n",
      "       ConvBlock-244            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-245            [-1, 832, 7, 7]               0\n",
      "          Conv2d-246            [-1, 128, 7, 7]         106,624\n",
      "     BatchNorm2d-247            [-1, 128, 7, 7]             256\n",
      "            ReLU-248            [-1, 128, 7, 7]               0\n",
      "       ConvBlock-249            [-1, 128, 7, 7]               0\n",
      " InceptionModule-250           [-1, 1024, 7, 7]               0\n",
      "AdaptiveAvgPool2d-251           [-1, 1024, 1, 1]               0\n",
      "         Dropout-252                 [-1, 1024]               0\n",
      "          Linear-253                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 7,013,112\n",
      "Trainable params: 7,013,112\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 118.72\n",
      "Params size (MB): 26.75\n",
      "Estimated Total Size (MB): 146.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
